{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os, pickle\n",
    "import time\n",
    "import argparse\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics.functional.classification import auroc, stat_scores, average_precision, precision_recall_curve, auc\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dwac_args import DWAC\n",
    "from dres_args import DRES\n",
    "from resn_args import RESN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Guassian kernel\n"
     ]
    }
   ],
   "source": [
    "args = argparse.Namespace(\n",
    "    train_dir='/net/scratch/hanliu/radiology/explain_teach/data/bm/train', \n",
    "    valid_dir='/net/scratch/hanliu/radiology/explain_teach/data/bm/valid', \n",
    "    eval_batch_size=1, embed_dim=10, merge_dim=2, merge_seq=True)\n",
    "name = 'emb10.merged2' \n",
    "# ckpt = '/net/scratch/hanliu/radiology/explain_teach/model/results/dwac-emb10-mrg10/1b62f0sd/checkpoints/epoch=81-valid_loss=0.20.ckpt' # DWAC\n",
    "ckpt = '/net/scratch/hanliu/radiology/explain_teach/model/results/dwac-emb10-mrg2/3n4ihavu/checkpoints/epoch=53-valid_loss=0.27.ckpt' # emb10-mrg2\n",
    "model = DWAC.load_from_checkpoint(ckpt, **vars(args))\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = list(iter(model.val_dataloader()))\n",
    "inputs = batches[0][0]\n",
    "labels = batches[0][1]\n",
    "# embeds = model.embed(inputs)\n",
    "# conv\n",
    "conv = [conv(inputs[:, i].unsqueeze(1).repeat(1, 16, 1, 1))\n",
    "                for i, conv in enumerate(model.conv)]\n",
    "embeds = torch.cat(conv, 1)\n",
    "# linear\n",
    "conv_x = torch.cat([c.unsqueeze(1) for c in conv], 1)\n",
    "linear = [linear[0](conv_x[:, i]) for i, linear in enumerate(model.linear)]\n",
    "fusion = model.fusion[0](torch.cat(conv, 1))\n",
    "embeds = torch.cat(linear + [fusion], 1)\n",
    "# merger\n",
    "# embeds = model.merger(embeds)\n",
    "\n",
    "batch = list(iter(model.ref_dataloader()))\n",
    "ref_x = batch[0][0]\n",
    "ref_y = batch[0][1]\n",
    "# ref_z = model.embed(ref_x)\n",
    "conv = [conv(ref_x[:, i].unsqueeze(1).repeat(1, 16, 1, 1))\n",
    "                for i, conv in enumerate(model.conv)]\n",
    "ref_z = torch.cat(conv, 1)\n",
    "# linear\n",
    "conv_x = torch.cat([c.unsqueeze(1) for c in conv], 1)\n",
    "linear = [linear[0](conv_x[:, i]) for i, linear in enumerate(model.linear)]\n",
    "fusion = model.fusion[0](torch.cat(conv, 1))\n",
    "ref_z = torch.cat(linear + [fusion], 1)\n",
    "# merger\n",
    "# ref_z = model.merger(ref_z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded valid embeddings (fids, inputs, labels, embeds) at /net/scratch/hanliu/radiology/explain_teach/data/bm/embs/dwac_valid_emb10.merged2.linear.0.pkl\n",
      "Encoded train embeddings (fids, inputs, labels, embeds) at /net/scratch/hanliu/radiology/explain_teach/data/bm/embs/dwac_train_emb10.merged2.linear.0.pkl\n"
     ]
    }
   ],
   "source": [
    "val_fids = sorted(os.listdir(model.hparams.valid_dir+'/0')) + sorted(os.listdir(model.hparams.valid_dir+'/1'))\n",
    "val_fids = [fid.replace('.npy', '') for fid in val_fids]\n",
    "\n",
    "ref_fids = sorted(os.listdir(model.hparams.train_dir+'/0')) + sorted(os.listdir(model.hparams.train_dir+'/1'))\n",
    "ref_fids = [fid.replace('.npy', '') for fid in ref_fids]\n",
    "\n",
    "val_fids = np.asarray(val_fids)\n",
    "inputs = np.asarray([i.squeeze().detach().numpy() for i in inputs])\n",
    "labels = np.asarray([l.squeeze().detach().numpy() for l in labels])\n",
    "embeds = np.asarray([e.squeeze().detach().numpy() for e in embeds])\n",
    "\n",
    "ref_fids = np.asarray(ref_fids)\n",
    "ref_x = np.asarray([i.squeeze().detach().numpy() for i in ref_x])\n",
    "ref_y = np.asarray([l.squeeze().detach().numpy() for l in ref_y])\n",
    "ref_z = np.asarray([e.squeeze().detach().numpy() for e in ref_z])\n",
    "\n",
    "path = model.hparams.valid_dir.replace('valid', 'embs/dwac_valid_{}.linear.0.pkl'.format(name))\n",
    "pickle.dump((val_fids, inputs, labels, embeds), open(path, \"wb\"))\n",
    "print(\"Encoded valid embeddings (fids, inputs, labels, embeds) at \" + path)\n",
    "\n",
    "path = model.hparams.train_dir.replace('train', 'embs/dwac_train_{}.linear.0.pkl'.format(name))\n",
    "pickle.dump((ref_fids, ref_x, ref_y, ref_z), open(path, \"wb\"))\n",
    "print(\"Encoded train embeddings (fids, inputs, labels, embeds) at \" + path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Guassian kernel\n"
     ]
    }
   ],
   "source": [
    "args = argparse.Namespace(\n",
    "    train_dir='/net/scratch/hanliu/radiology/explain_teach/data/bm/train', \n",
    "    valid_dir='/net/scratch/hanliu/radiology/explain_teach/data/bm/valid', \n",
    "    eval_batch_size=1, embed_dim=2)\n",
    "name = 'emb2' \n",
    "# ckpt = '/net/scratch/hanliu/radiology/explain_teach/model/results/dres-emb10/3r59qpvk/checkpoints/epoch=57-valid_loss=0.06.ckpt' # DRES\n",
    "# ckpt = '/net/scratch/hanliu/radiology/explain_teach/model/results/dres-emb10/257qh640/checkpoints/epoch=56-valid_loss=0.25.ckpt' # DRES\n",
    "ckpt = '/net/scratch/hanliu/radiology/explain_teach/model/results/dres-emb2/u15h6xun/checkpoints/epoch=34-valid_loss=0.19.ckpt' # emb2\n",
    "model = DRES.load_from_checkpoint(ckpt, **vars(args))\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = list(iter(model.val_dataloader()))\n",
    "inputs = batches[0][0]\n",
    "labels = batches[0][1]\n",
    "# embeds = model.embed(inputs)\n",
    "embeds = model.feature_extractor(inputs)\n",
    "embeds = model.fc[:4](embeds)\n",
    "\n",
    "batch = list(iter(model.ref_dataloader()))\n",
    "ref_x = batch[0][0]\n",
    "ref_y = batch[0][1]\n",
    "# ref_z = model.embed(ref_x)\n",
    "ref_z = model.feature_extractor(ref_x)\n",
    "ref_z = model.fc[:4](ref_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded valid embeddings (fids, inputs, labels, embeds) at /net/scratch/hanliu/radiology/explain_teach/data/bm/embs/dres_valid_emb2.linear.0.pkl\n",
      "Encoded train embeddings (fids, inputs, labels, embeds) at /net/scratch/hanliu/radiology/explain_teach/data/bm/embs/dres_train_emb2.linear.0.pkl\n"
     ]
    }
   ],
   "source": [
    "val_fids = sorted(os.listdir(model.hparams.valid_dir+'/0')) + sorted(os.listdir(model.hparams.valid_dir+'/1'))\n",
    "val_fids = [fid.replace('.npy', '') for fid in val_fids]\n",
    "\n",
    "ref_fids = sorted(os.listdir(model.hparams.train_dir+'/0')) + sorted(os.listdir(model.hparams.train_dir+'/1'))\n",
    "ref_fids = [fid.replace('.npy', '') for fid in ref_fids]\n",
    "\n",
    "val_fids = np.asarray(val_fids)\n",
    "inputs = np.asarray([i.squeeze().detach().numpy() for i in inputs])\n",
    "labels = np.asarray([l.squeeze().detach().numpy() for l in labels])\n",
    "embeds = np.asarray([e.squeeze().detach().numpy() for e in embeds])\n",
    "\n",
    "ref_fids = np.asarray(ref_fids)\n",
    "ref_x = np.asarray([i.squeeze().detach().numpy() for i in ref_x])\n",
    "ref_y = np.asarray([l.squeeze().detach().numpy() for l in ref_y])\n",
    "ref_z = np.asarray([e.squeeze().detach().numpy() for e in ref_z])\n",
    "\n",
    "path = model.hparams.valid_dir.replace('valid', 'embs/dres_valid_{}.linear.0.pkl'.format(name))\n",
    "pickle.dump((val_fids, inputs, labels, embeds), open(path, \"wb\"))\n",
    "print(\"Encoded valid embeddings (fids, inputs, labels, embeds) at \" + path)\n",
    "\n",
    "path = model.hparams.train_dir.replace('train', 'embs/dres_train_{}.linear.0.pkl'.format(name))\n",
    "pickle.dump((ref_fids, ref_x, ref_y, ref_z), open(path, \"wb\"))\n",
    "print(\"Encoded train embeddings (fids, inputs, labels, embeds) at \" + path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 'valid'\n",
    "args = argparse.Namespace(\n",
    "    valid_dir='/net/scratch/hanliu/radiology/explain_teach/data/bm/{}'.format(split), \n",
    "    eval_batch_size=1, embed_dim=2)\n",
    "name = 'emb2' \n",
    "# ckpt = '/net/scratch/hanliu/radiology/explain_teach/model/results/resn-emb10/32vzr4v5/checkpoints/epoch=95-valid_loss=0.30.ckpt' # RESN\n",
    "# ckpt = '/net/scratch/hanliu/radiology/explain_teach/model/results/resn-emb10/39e6y5lf/checkpoints/epoch=97-valid_loss=0.39.ckpt' # RESN\n",
    "ckpt = '/net/scratch/hanliu/radiology/explain_teach/model/results/resn-emb2/2pqtxzwi/checkpoints/epoch=72-valid_loss=0.50.ckpt' # emb2\n",
    "model = RESN.load_from_checkpoint(ckpt, **vars(args))\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = list(iter(model.val_dataloader()))\n",
    "inputs = [b[0] for b in batches]\n",
    "labels = [b[1] for b in batches]\n",
    "# embeds = [model.feature_extractor(im) for im in inputs]\n",
    "embeds = [model.fc[:4](model.feature_extractor(im)) for im in inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded valid findings (fids, inputs, labels, embeds) at /net/scratch/hanliu/radiology/explain_teach/data/bm/embs/resn_valid_emb2.linear.0.pkl\n"
     ]
    }
   ],
   "source": [
    "fids = sorted(os.listdir(model.hparams.valid_dir+'/0')) + sorted(os.listdir(model.hparams.valid_dir+'/1'))\n",
    "fids = [fid.replace('.jpg', '') for fid in fids]\n",
    "fids = np.asarray(fids)\n",
    "embeds = np.asarray([e.squeeze().detach().numpy() for e in embeds])[0]\n",
    "inputs = np.asarray([i.squeeze().detach().numpy() for i in inputs])[0]\n",
    "labels = np.asarray([l.squeeze().detach().numpy() for l in labels])[0]\n",
    "\n",
    "path = model.hparams.valid_dir.replace(split, 'embs/resn_{}_{}.linear.0.pkl'.format(split, name))\n",
    "pickle.dump((fids, inputs, labels, embeds), open(path, \"wb\"))\n",
    "print(\"Encoded {} findings (fids, inputs, labels, embeds) at \".format(split, name) + path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63211ff5f667d2462bf3cbef4ab188efe8fd5838e9505b6620d21c1e36f41af8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
