{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os, pickle\n",
    "import time\n",
    "import argparse\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics.functional.classification import auroc, stat_scores, average_precision, precision_recall_curve, auc\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from dwac_args import DWAC\n",
    "from dres_args import DRES\n",
    "from resn_args import RESN"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "args = argparse.Namespace(\n",
    "    train_dir='/net/scratch/hanliu/radiology/explain_teach/data/bm/train', \n",
    "    valid_dir='/net/scratch/hanliu/radiology/explain_teach/data/bm/valid', \n",
    "    eval_batch_size=1, embed_dim=10, merge_dim=10, merge_seq=True)\n",
    "name = 'emb10.merged10' \n",
    "ckpt = '/net/scratch/hanliu/radiology/explain_teach/model/results/dwac-emb10-mrg10/1b62f0sd/checkpoints/epoch=81-valid_loss=0.20.ckpt' # DWAC\n",
    "model = DWAC.load_from_checkpoint(ckpt, **vars(args))\n",
    "_ = model.eval()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using Guassian kernel\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "batches = list(iter(model.val_dataloader()))\n",
    "inputs = batches[0][0]\n",
    "labels = batches[0][1]\n",
    "embeds = model.embed(inputs)\n",
    "\n",
    "batch = list(iter(model.ref_dataloader()))\n",
    "ref_x = batch[0][0]\n",
    "ref_y = batch[0][1]\n",
    "ref_z = model.embed(ref_x)\n",
    "\n",
    "val_fids = sorted(os.listdir(model.hparams.valid_dir+'/0')) + sorted(os.listdir(model.hparams.valid_dir+'/1'))\n",
    "val_fids = [fid.replace('.npy', '') for fid in val_fids]\n",
    "\n",
    "ref_fids = sorted(os.listdir(model.hparams.train_dir+'/0')) + sorted(os.listdir(model.hparams.train_dir+'/1'))\n",
    "ref_fids = [fid.replace('.npy', '') for fid in ref_fids]\n",
    "\n",
    "val_fids = np.asarray(val_fids)\n",
    "inputs = np.asarray([i.squeeze().detach().numpy() for i in inputs])\n",
    "labels = np.asarray([l.squeeze().detach().numpy() for l in labels])\n",
    "embeds = np.asarray([e.squeeze().detach().numpy() for e in embeds])\n",
    "\n",
    "ref_fids = np.asarray(ref_fids)\n",
    "ref_x = np.asarray([i.squeeze().detach().numpy() for i in ref_x])\n",
    "ref_y = np.asarray([l.squeeze().detach().numpy() for l in ref_y])\n",
    "ref_z = np.asarray([e.squeeze().detach().numpy() for e in ref_z])\n",
    "\n",
    "path = model.hparams.valid_dir.replace('valid', 'embs/dwac_valid_{}.pkl'.format(name))\n",
    "pickle.dump((val_fids, inputs, labels, embeds), open(path, \"wb\"))\n",
    "print(\"Encoded valid embeddings (fids, inputs, labels, embeds) at \" + path)\n",
    "\n",
    "path = model.hparams.train_dir.replace('train', 'embs/dwac_train_{}.pkl'.format(name))\n",
    "pickle.dump((ref_fids, ref_x, ref_y, ref_z), open(path, \"wb\"))\n",
    "print(\"Encoded train embeddings (fids, inputs, labels, embeds) at \" + path)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Encoded valid embeddings (fids, inputs, labels, embeds) at /net/scratch/hanliu/radiology/explain_teach/data/bm/embs/dwac_valid_emb10.merged10.pkl\n",
      "Encoded train embeddings (fids, inputs, labels, embeds) at /net/scratch/hanliu/radiology/explain_teach/data/bm/embs/dwac_train_emb10.merged10.pkl\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "args = argparse.Namespace(\n",
    "    train_dir='/net/scratch/hanliu/radiology/explain_teach/data/bm/train', \n",
    "    valid_dir='/net/scratch/hanliu/radiology/explain_teach/data/bm/valid', \n",
    "    eval_batch_size=1, embed_dim=10)\n",
    "name = 'emb10' \n",
    "ckpt = '/net/scratch/hanliu/radiology/explain_teach/model/results/dres-emb10/3r59qpvk/checkpoints/epoch=57-valid_loss=0.06.ckpt' # DRES\n",
    "model = DRES.load_from_checkpoint(ckpt, **vars(args))\n",
    "_ = model.eval()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using Guassian kernel\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "batches = list(iter(model.val_dataloader()))\n",
    "inputs = batches[0][0]\n",
    "labels = batches[0][1]\n",
    "embeds = model.embed(inputs)\n",
    "\n",
    "batch = list(iter(model.ref_dataloader()))\n",
    "ref_x = batch[0][0]\n",
    "ref_y = batch[0][1]\n",
    "ref_z = model.embed(ref_x)\n",
    "\n",
    "val_fids = sorted(os.listdir(model.hparams.valid_dir+'/0')) + sorted(os.listdir(model.hparams.valid_dir+'/1'))\n",
    "val_fids = [fid.replace('.npy', '') for fid in val_fids]\n",
    "\n",
    "ref_fids = sorted(os.listdir(model.hparams.train_dir+'/0')) + sorted(os.listdir(model.hparams.train_dir+'/1'))\n",
    "ref_fids = [fid.replace('.npy', '') for fid in ref_fids]\n",
    "\n",
    "val_fids = np.asarray(val_fids)\n",
    "inputs = np.asarray([i.squeeze().detach().numpy() for i in inputs])\n",
    "labels = np.asarray([l.squeeze().detach().numpy() for l in labels])\n",
    "embeds = np.asarray([e.squeeze().detach().numpy() for e in embeds])\n",
    "\n",
    "ref_fids = np.asarray(ref_fids)\n",
    "ref_x = np.asarray([i.squeeze().detach().numpy() for i in ref_x])\n",
    "ref_y = np.asarray([l.squeeze().detach().numpy() for l in ref_y])\n",
    "ref_z = np.asarray([e.squeeze().detach().numpy() for e in ref_z])\n",
    "\n",
    "path = model.hparams.valid_dir.replace('valid', 'embs/drec_valid_{}.pkl'.format(name))\n",
    "pickle.dump((val_fids, inputs, labels, embeds), open(path, \"wb\"))\n",
    "print(\"Encoded valid embeddings (fids, inputs, labels, embeds) at \" + path)\n",
    "\n",
    "path = model.hparams.train_dir.replace('train', 'embs/drec_train_{}.pkl'.format(name))\n",
    "pickle.dump((ref_fids, ref_x, ref_y, ref_z), open(path, \"wb\"))\n",
    "print(\"Encoded train embeddings (fids, inputs, labels, embeds) at \" + path)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Encoded valid embeddings (fids, inputs, labels, embeds) at /net/scratch/hanliu/radiology/explain_teach/data/bm/embs/dwac_valid_emb10.pkl\n",
      "Encoded train embeddings (fids, inputs, labels, embeds) at /net/scratch/hanliu/radiology/explain_teach/data/bm/embs/dwac_train_emb10.pkl\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "split = 'valid'\n",
    "args = argparse.Namespace(\n",
    "    valid_dir='/net/scratch/hanliu/radiology/explain_teach/data/bm/{}'.format(split), \n",
    "    eval_batch_size=1, embed_dim=10)\n",
    "name = 'emb10' \n",
    "ckpt = '/net/scratch/hanliu/radiology/explain_teach/model/results/resn-emb10/32vzr4v5/checkpoints/epoch=95-valid_loss=0.30.ckpt' # RESN\n",
    "model = RESN.load_from_checkpoint(ckpt, **vars(args))\n",
    "_ = model.eval()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "batches = list(iter(model.val_dataloader()))\n",
    "inputs = [b[0] for b in batches]\n",
    "labels = [b[1] for b in batches]\n",
    "fids = sorted(os.listdir(model.hparams.valid_dir+'/0')) + sorted(os.listdir(model.hparams.valid_dir+'/1'))\n",
    "fids = [fid.replace('.jpg', '') for fid in fids]\n",
    "fids = np.asarray(fids)\n",
    "embeds = [model.embed(im) for im in inputs]\n",
    "embeds = np.asarray([e.squeeze().detach().numpy() for e in embeds])\n",
    "inputs = np.asarray([i.squeeze().detach().numpy() for i in inputs])\n",
    "labels = np.asarray([l.squeeze().detach().numpy() for l in labels])\n",
    "\n",
    "path = model.hparams.valid_dir.replace(split, 'embs/resn_{}_{}.pkl'.format(split, name))\n",
    "pickle.dump((fids, inputs, labels, embeds), open(path, \"wb\"))\n",
    "print(\"Encoded {} findings (fids, inputs, labels, embeds) at \".format(split, name) + path)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Encoded valid findings (fids, inputs, labels, embeds) at /net/scratch/hanliu/radiology/explain_teach/data/bm/embs/resn_valid_emb10.pkl\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "b00ecbf925dc872e170dec595415e7f1f8f24786438e6c412f2d7d150a701b85"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}