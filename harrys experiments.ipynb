{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os, pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torch\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import models.utils as utils\n",
    "import csv\n",
    "\n",
    "from torchvision import transforms\n",
    "import evals.embed_evals as evals\n",
    "import pickle\n",
    "import utils.plot_data as plot\n",
    "import shutil\n",
    "import utils.gen_triplets as gen\n",
    "import pandas as pd\n",
    "from omegaconf import OmegaConf as oc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_triplets.pkl',\n",
       " 'test_triplets_filtered.pkl',\n",
       " 'train_triplets.pkl',\n",
       " 'train_triplets_filtered.pkl',\n",
       " 'valid_triplets.pkl',\n",
       " 'valid_triplets_filtered.pkl']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = os.listdir(dir)\n",
    "a.sort()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "align=0.7\n",
      "10000\n",
      "9159\n",
      "8000\n",
      "7474\n",
      "2000\n",
      "1775\n",
      "align=0.775\n",
      "10000\n",
      "8466\n",
      "8000\n",
      "6969\n",
      "2000\n",
      "1742\n",
      "align=0.8\n",
      "10000\n",
      "8499\n",
      "8000\n",
      "6986\n",
      "2000\n",
      "1746\n",
      "align=0.85\n",
      "10000\n",
      "8485\n",
      "8000\n",
      "6977\n",
      "2000\n",
      "1755\n",
      "align=0.9\n",
      "10000\n",
      "8441\n",
      "8000\n",
      "6954\n",
      "2000\n",
      "1753\n",
      "align=0.95\n",
      "10000\n",
      "8633\n",
      "8000\n",
      "7086\n",
      "2000\n",
      "1786\n",
      "align=0.975\n",
      "10000\n",
      "9332\n",
      "8000\n",
      "7553\n",
      "2000\n",
      "1868\n",
      "align=1\n",
      "10000\n",
      "9381\n",
      "8000\n",
      "7617\n",
      "2000\n",
      "1877\n"
     ]
    }
   ],
   "source": [
    "for align in  [0.7, 0.775,0.8,0.85,0.9,0.95,0.975,1]:\n",
    "    dir = f\"align={str(align)}\"\n",
    "    print(dir)\n",
    "    dir = os.path.join(\"data/wv_2d_triplets\",dir)\n",
    "    files = os.listdir(dir)\n",
    "    files.sort()\n",
    "    for f in files:\n",
    "        f = os.path.join(dir,f)\n",
    "        print(len(pickle.load(open(f,\"rb\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triplets = pickle.load(open(\"data/wv_2d_triplets/align=0.7/train_triplets.pkl\", \"rb\"))\n",
    "valid_triplets = pickle.load(open(\"data/wv_2d_triplets/align=0.7/valid_triplets.pkl\", \"rb\"))\n",
    "test_triplets = pickle.load(open(\"data/wv_2d_triplets/align=0.7/test_triplets.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train = pickle.load(open(\"embeds/wv_2d/train.pkl\", \"rb\"))\n",
    "x_test = pickle.load(open(\"embeds/wv_2d/test.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2s = np.arange(0,1,0.0001)\n",
    "y = [evals.distorted_1nn(x_train, y_train, x_test, y_test, [1,w2],[2,2]) for w2 in w2s]\n",
    "plt.plot(w2s,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2=0.5\n",
    "evals.distorted_1nn(x_train, y_train, x_test, y_test, [1,w2],[2,2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2=0.5\n",
    "evals.distorted_1nn(x_train, y_train, x_test, y_test, [1,w2],[2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals.distorted_1nn(x_train, y_train, x_test, y_test, [1,-2],[2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for align in [0.775, 0.8, 0.85, 0.9, 0.95, 0.975, 1]:\n",
    "    path = os.path.join(\"data/wv_2d_triplets\",f\"align={str(align)}\")\n",
    "    os.mkdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vis results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(\"models/results.csv\")\n",
    "cols = [\"test_triplet_acc\", \"test_1nn_acc\", \"decision_support\"]\n",
    "TN_df = results.loc[results[\"wandb_group\"]==\"TN_test\"]\n",
    "TN_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(9), TN_df[\"test_triplet_acc\"][::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(\"models/results.csv\")\n",
    "cols = [\"test_triplet_acc\", \"test_1nn_acc\", \"decision_support\"]\n",
    "MTL_df = results.loc[results[\"wandb_group\"]==\"MTL0.8\"]\n",
    "MTL_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESN_df = pd.read_csv(\"/net/scratch/tianh/explain_teach/models/results_csv/new_resn/wv_RESN.csv\")\n",
    "TN_df = pd.read_csv(\"/net/scratch/tianh/explain_teach/models/results_csv/new_resn/wv_TN.csv\")\n",
    "MTL_df = pd.read_csv(\"/net/scratch/tianh/explain_teach/models/results_csv/new_resn/wv_MTL.csv\")\n",
    "plots = [\"test_triplet_acc\", \"test_1nn_acc\", \"decision_support\"]\n",
    "lw=3\n",
    "for plot in plots:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(RESN_df[\"align\"], RESN_df[plot], lw=lw)\n",
    "    plt.plot(TN_df[\"align\"], TN_df[plot], lw=lw)\n",
    "    plt.plot(MTL_df[\"align\"], MTL_df[plot], lw=lw)\n",
    "    plt.title(plot)\n",
    "    plt.legend([\"RESN\", \"TN\", \"MTL\"])\n",
    "    plt.xticks(RESN_df[\"align\"])\n",
    "    plt.xlabel(\"alignment\")\n",
    "    plt.ylim((0,1))\n",
    "    plt.savefig(f\"figs/wv{plot}.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_triplets = []\n",
    "for triplet in stt:\n",
    "    test, t1, t2 = triplet\n",
    "    triplet = [test_img2idx[test], train_img2idx[t1], train_img2idx[t2]]\n",
    "    test_triplets.append(triplet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(test_triplets, open(\"/net/scratch/tianh/explain_teach/data/bm_triplets/test_triplets.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_f = utils.dataset_filenames(\"/net/scratch/tianh-shared/bm/test\")[:,0]\n",
    "test_img2idx = {x.split(\"/\")[-1]:i for i,x in enumerate(test_f)}\n",
    "pickle.dump(test_img2idx, open(\"/net/scratch/tianh/explain_teach/data/img2idx/bm_test.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(data={\"wandb_group\": [0], \"wandb_name\": [0],\n",
    "    \"test_clf_acc\": [0], \"test_clf_loss\": [0], \"test_1nn_acc\": [0], \"test_triplet_acc\":[0], \"decision_support\": [0]})\n",
    "df.to_csv(\"models/results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### triplet gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeds = pickle.load(open(\"/net/scratch/tianh/explain_teach/embeds/wv_2d/train.pkl\",\"rb\"))\n",
    "valid_embeds = pickle.load(open(\"/net/scratch/tianh/explain_teach/embeds/wv_2d/valid.pkl\",\"rb\"))\n",
    "test_embeds = pickle.load(open(\"/net/scratch/tianh/explain_teach/embeds/wv_2d/test.pkl\",\"rb\"))\n",
    "len_train = len(train_embeds)\n",
    "len_valid = len(valid_embeds)\n",
    "len_test = len(test_embeds)\n",
    "y_train = np.array([x[1] for x in torchvision.datasets.ImageFolder(\"data/weevil_vespula/train\")])\n",
    "y_valid = np.array([x[1] for x in torchvision.datasets.ImageFolder(\"data/weevil_vespula/valid\")])\n",
    "y_test = np.array([x[1] for x in torchvision.datasets.ImageFolder(\"data/weevil_vespula/test\")])\n",
    "classes = [0,1]\n",
    "y_train_idx = [np.where(y_train==c)[0] for c in classes]\n",
    "y_valid_idx = [np.where(y_valid==c)[0] for c in classes]\n",
    "y_test_idx = [np.where(y_test==c)[0] for c in classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2s = [0.0001,0.001,0.0025,0.005,0.05,0.5,1]\n",
    "aligns = [0.775,0.8,0.85,0.9,0.95,0.975,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6969 1742 8466\n",
      "6986 1746 8499\n",
      "6977 1755 8485\n",
      "6954 1753 8441\n",
      "7086 1786 8633\n",
      "7553 1868 9332\n",
      "7617 1877 9381\n"
     ]
    }
   ],
   "source": [
    "for w2,align in zip(w2s, aligns):\n",
    "    dir = f\"align={str(align)}\"\n",
    "    weights = [1,w2]\n",
    "    total = 10000\n",
    "    train_triplets = gen.sample_triplets(np.arange(len_train), int(total*0.8), weights, train_embeds, [2,2])\n",
    "    valid_triplets = gen.sample_mixed_triplets(np.arange(len_valid),np.arange(len_train), int(total*0.2), weights, valid_embeds, train_embeds, [2,2])\n",
    "    test_triplets = gen.sample_mixed_triplets(np.arange(len_test),np.arange(len_train), total, weights, test_embeds, train_embeds, [2,2])\n",
    "    pickle.dump(train_triplets, open(f\"/net/scratch/tianh/explain_teach/data/wv_2d_triplets/{dir}/train_triplets.pkl\", \"wb\"))\n",
    "    pickle.dump(valid_triplets, open(f\"/net/scratch/tianh/explain_teach/data/wv_2d_triplets/{dir}/valid_triplets.pkl\", \"wb\"))\n",
    "    pickle.dump(test_triplets, open(f\"/net/scratch/tianh/explain_teach/data/wv_2d_triplets/{dir}/test_triplets.pkl\", \"wb\"))\n",
    "    train_triplets_filtered = []\n",
    "    valid_triplets_filtered = []\n",
    "    test_triplets_filtered = []\n",
    "    for t in train_triplets:\n",
    "        a,p,n = t\n",
    "        if not ((a in y_train_idx[0] and p in y_train_idx[1] and n in y_train_idx[0]) or (a in y_train_idx[1] and p in y_train_idx[0] and n in y_train_idx[1])): train_triplets_filtered.append(t)\n",
    "    for t in valid_triplets:\n",
    "        a,p,n = t\n",
    "        if not ((a in y_valid_idx[0] and p in y_train_idx[1] and n in y_train_idx[0]) or (a in y_valid_idx[1] and p in y_train_idx[0] and n in y_train_idx[1])): valid_triplets_filtered.append(t)\n",
    "    for t in test_triplets:\n",
    "        a,p,n = t\n",
    "        if not ((a in y_test_idx[0] and p in y_train_idx[1] and n in y_train_idx[0]) or (a in y_test_idx[1] and p in y_train_idx[0] and n in y_train_idx[1])): test_triplets_filtered.append(t)\n",
    "    print(len(train_triplets_filtered), len(valid_triplets_filtered), len(test_triplets_filtered))\n",
    "    pickle.dump(train_triplets_filtered, open(f\"data/wv_2d_triplets/{dir}/train_triplets_filtered.pkl\",\"wb\"))\n",
    "    pickle.dump(valid_triplets_filtered, open(f\"data/wv_2d_triplets/{dir}/valid_triplets_filtered.pkl\",\"wb\"))\n",
    "    pickle.dump(test_triplets_filtered, open(f\"data/wv_2d_triplets/{dir}/test_triplets_filtered.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[26, 70, 52]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = oc.load(\"models/configs/wv_2d/triplets/align=0.7.yaml\")\n",
    "base_filtered = oc.load(\"models/configs/wv_2d/triplets/align=0.7_filtered.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w2,align in zip(w2s, aligns):\n",
    "    weights = [1,w2]\n",
    "    train_triplets = base.train_triplets.replace(\"0.7\",str(align))\n",
    "    valid_triplets = base.valid_triplets.replace(\"0.7\",str(align))\n",
    "    test_triplets = base.test_triplets.replace(\"0.7\",str(align))\n",
    "    wandb_group = base.wandb_group.replace(\"0.7\",str(align))\n",
    "    overwrite = {\"weights\":weights, \"train_triplets\":train_triplets,\"valid_triplets\":valid_triplets,\n",
    "    \"test_triplets\":test_triplets,\"wandb_group\":wandb_group}\n",
    "    oc.save(oc.merge(base,overwrite), f\"models/configs/wv_2d/triplets/align={align}.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w2,align in zip(w2s, aligns):\n",
    "    weights = [1,w2]\n",
    "    train_triplets = base_filtered.train_triplets.replace(\"0.7\",str(align))\n",
    "    valid_triplets = base_filtered.valid_triplets.replace(\"0.7\",str(align))\n",
    "    test_triplets = base_filtered.test_triplets.replace(\"0.7\",str(align))\n",
    "    wandb_group = base_filtered.wandb_group.replace(\"0.7\",str(align))\n",
    "    overwrite = {\"weights\":weights, \"train_triplets\":train_triplets,\"valid_triplets\":valid_triplets,\n",
    "    \"test_triplets\":test_triplets,\"wandb_group\":wandb_group}\n",
    "    oc.save(oc.merge(base_filtered,overwrite), f\"models/configs/wv_2d/triplets/align={align}_filtered.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1, c2, c3, c4 = np.arange(1583), np.arange(1583,3166), np.arange(600,900), np.arange(900,1200)\n",
    "splits = utils.cross_val_multiclass([c1,c2], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = np.arange(1583)\n",
    "valid_test = np.random.choice(c1, 300,replace=False)\n",
    "c1_valid = np.random.choice(valid_test, 150, replace=False)\n",
    "c1_test = np.setdiff1d(valid_test, c1_valid)\n",
    "c1_train = np.setdiff1d(c1, valid_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2 = np.arange(1583,5856)\n",
    "valid_test = np.random.choice(c2, 300,replace=False)\n",
    "c2_valid = np.random.choice(valid_test, 150, replace=False)\n",
    "c2_test = np.setdiff1d(valid_test, c2_valid)\n",
    "c2_train = np.setdiff1d(c2, valid_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(splits,open(\"models/pneumonia_splits_imb.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = \"/net/scratch/tianh-shared/warblers/inat/4class\"\n",
    "dst_dir = \"/net/scratch/tianh-shared/warblers/data\"\n",
    "split5 = pickle.load(open(\"models/bird_splits.pkl\",\"rb\"))[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = split5\n",
    "instances = utils.dataset_filenames(src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [v[1] for v in instances[train]]\n",
    "unique = set(labels)\n",
    "counts = [labels.count(c) for c in unique]\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.arange(0.1, 1, 0.1)\n",
    "clf = [0.875, 0.925, 0.95, 0.975, 0.975, 0.975, 1, 1, 0.975]\n",
    "triplet = [0.897,0.913,0.913,0.855,0.841,0.826,0.841,0.841,0.841]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lw=4\n",
    "plt.rc('legend', fontsize=15)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.axhline(1, c='black',linewidth=lw, label=\"ResNET clf acc\")\n",
    "plt.plot(lambdas, clf,linewidth=lw, label=\"MTL clf acc\")\n",
    "plt.plot(lambdas, triplet,linewidth=lw, label=\"MTL triplet acc\")\n",
    "plt.axhline(0.841, c='green',linewidth=lw, label=\"TN triplet acc\")\n",
    "plt.xlabel('lambda')\n",
    "# plt.title(\"MTL trained on human triplets\")\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.1, -0.1),fancybox=True, shadow=True, ncol=4)\n",
    "plt.savefig(\"figs/MTL_lambdas.pdf\", format=\"pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "triplets = pickle.load(open(\"/net/scratch/tianh/triplet-webapp/backup/trial0/butterfly.triplets.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = pickle.load(open(\"data/bm_train_idx.pkl\", \"rb\"))[1]\n",
    "val_idx  = pickle.load(open(\"data/bm_valid_idx.pkl\", \"rb\"))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triplets = []\n",
    "val_triplets = []\n",
    "for triplet in triplets:\n",
    "    a,p,n = triplet\n",
    "    if a in train_idx.keys() and p in train_idx.keys() and n in train_idx.keys():\n",
    "        new = [train_idx[a], train_idx[p], train_idx[n]]\n",
    "        train_triplets.append(new)\n",
    "    elif a in val_idx.keys() and p in val_idx.keys() and n in val_idx.keys():\n",
    "        new = [val_idx[a], val_idx[p], val_idx[n]]\n",
    "        val_triplets.append(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_triplets = \"/net/scratch/tianh/explain_teach/data/bm_triplets/3c2_unique=182/train_triplets.pkl\"\n",
    "valid_triplets = \"/net/scratch/tianh/explain_teach/data/bm_triplets/3c2_unique=182/valid_triplets.pkl\"\n",
    "train_triplets = pickle.load(open(train_triplets, \"rb\"))\n",
    "valid_triplets = pickle.load(open(valid_triplets, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_lpips_distance = \"/net/scratch/tianh/explain_teach/embeds/lpips/lpips.bm.train.pkl\"\n",
    "valid_lpips_distance = \"/net/scratch/tianh/explain_teach/embeds/lpips/lpips.bm.valid.pkl\"\n",
    "train_lpips_distance = pickle.load(open(train_lpips_distance, \"rb\"))\n",
    "valid_lpips_distance = pickle.load(open(valid_lpips_distance, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for htriplet in train_triplets:\n",
    "    total += 1\n",
    "    a,p,n = htriplet[0], htriplet[1], htriplet[2]\n",
    "    if train_lpips_distance[a, p] < train_lpips_distance[a, n]:\n",
    "        correct += 1\n",
    "correct/total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for htriplet in valid_triplets:\n",
    "    total += 1\n",
    "    a,p,n = htriplet[0], htriplet[1], htriplet[2]\n",
    "    if valid_lpips_distance[a, p] < valid_lpips_distance[a, n]:\n",
    "        correct += 1\n",
    "correct/total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_pairwise_distance= \"embeds/lpips.bm.train.pkl\" \n",
    "# valid_pairwise_distance= \"embeds/lpips.bm.valid.pkl\" \n",
    "# train_pairwise_distance = pickle.load(open(train_pairwise_distance, \"rb\"))\n",
    "# valid_pairwise_distance = pickle.load(open(valid_pairwise_distance, \"rb\"))\n",
    "\n",
    "train_triplets = \"/net/scratch/tianh/explain_teach/data/bm_triplets/train_triplets.pkl\"\n",
    "valid_triplets = \"/net/scratch/tianh/explain_teach/data/bm_triplets/valid_triplets.pkl\"\n",
    "train_triplets = pickle.load(open(train_triplets, \"rb\"))\n",
    "valid_triplets = pickle.load(open(valid_triplets, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pickle.load(open(\"embeds/TN_bm_train.pkl\",\"rb\"))\n",
    "valid = pickle.load(open(\"embeds/TN_bm_valid.pkl\",\"rb\"))\n",
    "triplets = pickle.load(open(\"data/bm_triplets/val2train_triplets.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = evals.val2train_triplet_acc(train, valid, triplets)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = evals.triplet_acc(train, train_triplets)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = evals.triplet_acc(valid, valid_triplets)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_triplets = \"/net/scratch/tianh/bm/triplets/train_triplets.pkl\"\n",
    "valid_triplets = \"/net/scratch/tianh/bm/triplets/valid_triplets.pkl\"\n",
    "# clf_train_triplets = \"/net/scratch/tianh/bm/triplets/clf_train_triplets.pkl\"\n",
    "# clf_valid_triplets = \"/net/scratch/tianh/bm/triplets/clf_valid_triplets.pkl\"\n",
    "\n",
    "train_triplets = pickle.load(open(train_triplets, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dir = \"/net/scratch/hanliu-shared/data/bm/train\"\n",
    "valid_dir = \"/net/scratch/hanliu-shared/data/bm/valid\"\n",
    "train_dataset = torchvision.datasets.ImageFolder(train_dir, transform=utils.bm_transform())\n",
    "valid_dataset = torchvision.datasets.ImageFolder(valid_dir, transform=utils.bm_transform())\n",
    "train_inputs = torch.tensor(np.array([data[0].numpy() for data in train_dataset]))\n",
    "valid_inputs = torch.tensor(np.array([data[0].numpy() for data in valid_dataset]))\n",
    "train_labels = torch.tensor(np.array([data[1] for data in train_dataset]))\n",
    "valid_labels = torch.tensor(np.array([data[1] for data in valid_dataset]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(train_labels==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pickle.load(open(\"/net/scratch/tianh/bm/triplets/clf_train_triplets.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.choice(len(combs), 32, replace=False)\n",
    "sample = combs[idx]\n",
    "len(np.unique(sample.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.tensor(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.unique(torch.flatten(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_idx = []\n",
    "for c in combs:\n",
    "    x1, x2, x3 = c[0], c[1], c[2]\n",
    "    if train_pairwise_distance[x1, x2] > train_pairwise_distance[x1, x3]:\n",
    "        triplet_idx.append([x1, x3, x2])\n",
    "    else:\n",
    "        triplet_idx.append([x1, x2, x3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(np.array(triplet_idx), open('/net/scratch/tianh/bm/triplets/train_triplets.pkl',\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pickle.load(open('/net/scratch/tianh/bm/triplets/train_triplets.pkl',\"rb\"))\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(triplet_idx).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food100 = []\n",
    "with open('/net/scratch/tianh/food100-dataset/all-triplets.csv', newline='') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "    for row in spamreader:\n",
    "        food100.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize([230,230]),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "data_dir = '/net/scratch/tianh/food100-dataset/images'\n",
    "dataset = torchvision.datasets.ImageFolder(data_dir, transform=transform)\n",
    "total_idx = np.arange(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_idx = np.random.choice(total_idx, int(len(total_idx)*0.615), replace=False)\n",
    "valid_img_idx = np.setdiff1d(total_idx, train_img_idx)\n",
    "\n",
    "train_data = torch.tensor(np.array([dataset[i][0].numpy() for i in train_img_idx]))\n",
    "valid_data = torch.tensor(np.array([dataset[i][0].numpy() for i in valid_img_idx]))\n",
    "train_label = torch.tensor(np.array([dataset[i][1] for i in train_img_idx]))\n",
    "valid_label = torch.tensor(np.array([dataset[i][1] for i in valid_img_idx]))\n",
    "\n",
    "valid_triplets = []\n",
    "for t in triplets:\n",
    "    if t[0] in valid_label and t[1] in valid_label and t[2] in valid_label:\n",
    "        valid_triplets.append(t)\n",
    "train_triplets = []\n",
    "for t in triplets:\n",
    "    if t[0] in train_label and t[1] in train_label and t[2] in train_label:\n",
    "        train_triplets.append(t)\n",
    "len(train_triplets), len(valid_triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "41685/11018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(triplets)*0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets = pickle.load(open(\"/net/scratch/tianh/food100-dataset/triplets_idx.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_idx = np.arange(100)\n",
    "train_idx = np.random.choice(total_idx, len(total_idx)*8//10, replace=False)\n",
    "valid_idx = np.setdiff1d(total_idx, train_idx)\n",
    "train_triplets = 0\n",
    "valid_triplets = 0\n",
    "for t in triplets:\n",
    "    if t[0] in train_idx:\n",
    "        train_triplets += 1\n",
    "    else:\n",
    "        valid_triplets += 1\n",
    "train_triplets/len(triplets), valid_triplets/len(triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/net/scratch/tianh/food100-dataset/images\"\n",
    "dir_list = os.listdir(path)\n",
    "dir_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "for image in dir_list:\n",
    "    image_dir = os.path.join(path, image[:-4]) \n",
    "    if not os.path.exists(image_dir):\n",
    "        os.mkdir(image_dir) \n",
    "        shutil.copy(os.path.join(path,image),image_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluating lpips(bm) by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for i in range(80):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for j in range(80):\n",
    "        for k in range(80,160):\n",
    "            total += 1\n",
    "            if train_dist[i,j] <= train_dist[i,k]:\n",
    "                correct += 1\n",
    "    scores.append(correct/total)\n",
    "    \n",
    "for i in range(80, 160):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for j in range(80):\n",
    "        for k in range(80,160):\n",
    "            total += 1\n",
    "            if train_dist[i,j] >=  train_dist[i,k]:\n",
    "                correct += 1\n",
    "    scores.append(correct/total)\n",
    "sum(scores)/len(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluating lpips(food) w/ triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_matrix = pickle.load(open(\"lpips.food.pkl\", \"rb\"))\n",
    "triplets = pickle.load(open(\"/net/scratch/tianh/food100-dataset/triplets_idx.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(triplets)\n",
    "correct = 0\n",
    "for triplet in triplets:\n",
    "    if distances_matrix[triplet[0], triplet[1]] <= distances_matrix[triplet[0], triplet[2]]:\n",
    "        correct += 1\n",
    "correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## triplet generalization error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import euclidean_distances \n",
    "import torch\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "pdist = torch.nn.PairwiseDistance()\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "np.random.seed(42)\n",
    "# def euc_dist(x, y): return euclidean_distances([x],[y])[0][0]\n",
    "def euc_dist(x, y): return np.sqrt(np.dot(x, x) - 2 * np.dot(x, y) + np.dot(y, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a(f):\n",
    "    return f(3)\n",
    "def f(x): return x+1\n",
    "a(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tste\n",
    "triplets = \"/net/scratch/tianh/bm/triplets/train_triplets.pkl\"\n",
    "triplets = np.array(pickle.load(open(triplets, \"rb\")))\n",
    "embedding = tste.tste(triplets, no_dims=2, verbose=False, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_align = []\n",
    "for triplet in tqdm(triplets):\n",
    "    a, p, n = triplet\n",
    "    ap = euc_dist(embedding[a], embedding[p]) \n",
    "    an = euc_dist(embedding[a], embedding[n])\n",
    "    train_align.append(ap < an)\n",
    "print(np.mean(train_align))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets = np.array(pickle.load(open(\"/net/scratch/tianh/food100-dataset/triplets_idx.pkl\", \"rb\")))\n",
    "valid_path = \"model/embeds/triplet_net_food_triplet.pkl\"\n",
    "X_train = pickle.load(open(valid_path, \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_align = []\n",
    "for triplet in tqdm(triplets):\n",
    "    a, p, n = triplet\n",
    "    ap = euc_dist(X_train[a], X_train[p]) \n",
    "    an = euc_dist(X_train[a], X_train[n])\n",
    "    train_align.append(ap < an)\n",
    "print(np.mean(train_align))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'triplet_net_food'\n",
    "name = 'no_linear'\n",
    "train_path = \"/net/scratch/hanliu-shared/data/bm/embs/dwac_train_emb10.merged2.pkl\"\n",
    "valid_path = \"/net/scratch/hanliu-shared/data/bm/embs/dwac_valid_emb10.merged2.pkl\"\n",
    "# train_path = '{}/{}_train_{}.pkl'.format(\"embeds\", model, name)\n",
    "# valid_path = '{}/{}_valid_{}.pkl'.format(\"embeds\", model, name)\n",
    "X_train = pickle.load(open(train_path, \"rb\"))\n",
    "X_train = X_train[3]\n",
    "X_valid = pickle.load(open(valid_path, \"rb\"))\n",
    "X_valid=X_valid[3]\n",
    "# valid_path = 'model/xd.pkl'\n",
    "# X_train = pickle.load(open(valid_path, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_align = []\n",
    "train_dist = pickle.load(open(\"embeds/lpips.bm.train.pkl\", \"rb\"))\n",
    "# train_dist = train_dist[np.array(subset_idx)]\n",
    "combs = torch.combinations(torch.arange(0, len(train_dist)).int(), r=3)\n",
    "for c in tqdm(combs):\n",
    "    a, p, n = c\n",
    "# for i in tqdm(range(10000)):\n",
    "#     a, p, n = np.random.choice(len(X_train), 3, replace=False)\n",
    "    ap = train_dist[a, p] < train_dist[a, n]\n",
    "    rd = euc_dist(X_train[a], X_train[p]) < euc_dist(X_train[a], X_train[n])\n",
    "    train_align.append(ap == rd)\n",
    "print(np.mean(train_align))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_align = []\n",
    "valid_dist = pickle.load(open(\"embeds/lpips.bm.valid.pkl\", \"rb\"))\n",
    "combs = torch.combinations(torch.arange(0, len(valid_dist)-1).int(), r=3)\n",
    "for c in combs:\n",
    "    a, p, n = c\n",
    "    ap = valid_dist[a, p] < valid_dist[a, n]\n",
    "    rd = euc_dist(X_valid[a], X_valid[p]) < euc_dist(X_valid[a], X_valid[n])\n",
    "    valid_align.append(ap == rd)\n",
    "print(np.mean(valid_align))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ap = pdist(torch.tensor(X_valid[combs[:,0]]), torch.tensor(X_valid[combs[:,1]]))\n",
    "d_an = pdist(torch.tensor(X_valid[combs[:,0]]), torch.tensor(X_valid[combs[:,2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(d_ap < d_an).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## human-compatible example selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, name = 'dwac', 'emb10.merged2'\n",
    "title = \"prostatex\"\n",
    "\n",
    "train_path = 'embeds/{}_train_{}.pkl'.format(model, name)\n",
    "f_train_dwac, y_train_dwac, x_train_dwac = pickle.load(open(train_path, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, pdash_examples, _ = pdash(x_train_dwac,x_train_dwac,10,kernelType=\"Gaussian\")\n",
    "pdash_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dwac-lpips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpips = pickle.load(open(\"embeds/lpips.prostatex.train+valid.pkl\",\"rb\"))\n",
    "lpips.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, examples, _ = pdash_human.pdash(x_train_dwac,x_train_dwac, 10,lpips,kernelType=\"Gaussian\")\n",
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, examples, _ = pdash_human.pdash(x_train_dwac,x_train_dwac, 10,lpips, f_h_scale=0.1, kernelType=\"Gaussian\")\n",
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, examples, _ = pdash_human.pdash(x_train_dwac,x_train_dwac, 10,lpips, f_h_scale=1, kernelType=\"Gaussian\")\n",
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.vis_proto(x_train_dwac, y_train_dwac, examples, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.vis_proto(x_train_dwac, y_train_dwac, pdash_examples, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 0.5\n",
    "Kernel = 'Gaussian'\n",
    "Gamma = 0.5\n",
    "k_range = [1, 3]\n",
    "m_range = list(range(3, 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'prostatex'\n",
    "model = 'dwac'\n",
    "name = 'emb10.merged2'\n",
    "train_path = 'embeds/{}_train_{}.pkl'.format(model, name)\n",
    "valid_path = 'embeds/{}_valid_{}.pkl'.format(model, name)\n",
    "f_train, y_train, X_train = pickle.load(open(train_path, \"rb\"))\n",
    "f_valid, y_valid, X_valid = pickle.load(open(valid_path, \"rb\"))\n",
    "data = X_train, y_train, X_valid, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwac_f_scores_knn, dwac_f_score_svm = utils.get_full_score(data, k_range)\n",
    "dwac_r_means_knn, dwac_r_confs_knn, dwac_r_means_svm, dwac_r_confs_svm = utils.get_random_score(data, k_range, m_range)\n",
    "dwac_p_scores_knn, dwac_p_scores_svm = utils.get_protodash_score(data, k_range, m_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdash_score(data, k_range, m_range, f_h_scale):\n",
    "    p_idss = {}\n",
    "    X_train, y_train, X_valid, y_valid = data\n",
    "    for m in m_range:\n",
    "        if m not in p_idss:\n",
    "            try:\n",
    "                _, index, _ = pdash_human.pdash(X_train,X_train, m,lpips, f_h_scale=f_h_scale, kernelType=\"Gaussian\")\n",
    "            except AttributeError:\n",
    "                index = [0] * m\n",
    "                print(\"error for m={}\".format(m))\n",
    "            p_idss[m] = index\n",
    "    p_scores_knn, p_scores_svm = [], []\n",
    "    for k in k_range:\n",
    "        for m in m_range:\n",
    "            p_scores_knn.append(utils.get_knn_score(k, data, p_idss[m]))\n",
    "            try:\n",
    "                s = utils.get_svm_score(k, data, p_idss[m])\n",
    "            except:\n",
    "                s = 0\n",
    "            p_scores_svm.append(s)\n",
    "    p_scores_knn = np.array(p_scores_knn).reshape(len(k_range), len(m_range))\n",
    "    p_scores_svm = np.array(p_scores_svm).reshape(len(k_range), len(m_range))\n",
    "    return p_scores_knn, p_scores_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdash_scores_knn1,pdash_scores_svm = get_pdash_score(data, k_range, m_range, 1)\n",
    "pdash_scores_knn01,pdash_scores_svm = get_pdash_score(data, k_range, m_range, 0.1)\n",
    "pdash_scores_knn001,pdash_scores_svm = get_pdash_score(data, k_range, m_range, 0.01)\n",
    "pdash_scores_knn0001,pdash_scores_svm = get_pdash_score(data, k_range, m_range, 0.001)\n",
    "pdash_scores_knn00001,pdash_scores_svm = get_pdash_score(data, k_range, m_range, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, len(k_range), figsize=(16, 6), sharey=True)\n",
    "for k in range(len(k_range)):\n",
    "    ax[k].axhline(dwac_f_scores_knn[k], c='black')\n",
    "    ax[k].plot(m_range, dwac_r_means_knn[k])\n",
    "    ax[k].fill_between(m_range, dwac_r_means_knn[k] + dwac_r_confs_knn[k] / 2, dwac_r_means_knn[k] - dwac_r_confs_knn[k] / 2, alpha=0.5)\n",
    "    ax[k].plot(m_range, dwac_p_scores_knn[k])\n",
    "    # ax[k].plot(m_range, pdash_scores_knn1[k])\n",
    "    ax[k].plot(m_range, pdash_scores_knn01[k])\n",
    "    ax[k].plot(m_range, pdash_scores_knn001[k])\n",
    "    # ax[k].plot(m_range, pdash_scores_knn0001[k])\n",
    "    # ax[k].plot(m_range, pdash_scores_knn00001[k])\n",
    "    ax[k].set_ylim(0.3, 1)\n",
    "    ax[k].set_xticks(m_range)\n",
    "    ax[k].set_title('K={}'.format(k_range[k]))\n",
    "    ax[k].legend(['full', 'random', '', 'protodash', '1NN_constraint', 'pdash_h','0.0001'])\n",
    "fig.suptitle('{}, {}.{}, AUC, KNN weights '.format(dataset, model, \"lpips\") + \"uniform\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate triplets from LPIPS distance matrices\n",
    "triplets.shape := [k, 3]\n",
    "\n",
    "triplets[i] = [anchor, positive, negative]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_matrix = pickle.load(open(\"lpips.prostatex.train+valid.pkl\", \"rb\"))\n",
    "distance_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets_fname = \"data/triplets.px.train+valid.pkl\"\n",
    "tste_fname = \"data/tste.px.train+valid.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeds = utils.get_tste(distance_matrix, triplets_fname, tste_fname, max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, name = 'dwac', 'emb10.merged2'\n",
    "title = \"prostatex\"\n",
    "\n",
    "train_path = '{}_train_{}.pkl'.format(model, name)\n",
    "f_train_dwac, y_train_dwac, x_train_dwac = pickle.load(open(train_path, \"rb\"))\n",
    "valid_path = '{}_valid_{}.pkl'.format(model, name)\n",
    "f_valid_dwac, y_valid_dwac, x_valid_dwac = pickle.load(open(valid_path, \"rb\"))\n",
    "vis_data(x_train_dwac, y_train_dwac, x_valid_dwac, y_valid_dwac, title, save=False)\n",
    "x_train_dwac.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"prostatex, tste\"\n",
    "model, name = \"tste\", \"px\"\n",
    "\n",
    "train_path = '{}_train_{}.pkl'.format(model, name)\n",
    "f_train_tste, y_train_tste, x_train_tste = pickle.load(open(train_path, \"rb\"))\n",
    "valid_path = '{}_valid_{}.pkl'.format(model, name)\n",
    "f_valid_tste, y_valid_tste, x_valid_tste = pickle.load(open(valid_path, \"rb\"))\n",
    "vis_data(x_train_tste, y_train_tste, x_valid_tste, y_valid_tste, title, save=False)\n",
    "x_train_tste.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_snack = np.load(\"embeds/snack_dwac+lpips.npy\")\n",
    "y_snack_path = \"embeds/tste_all_px.pkl\"\n",
    "y_snack = pickle.load(open(y_snack_path, \"rb\"))[1]\n",
    "assert (x_snack.shape[0]==y_snack.shape[0])\n",
    "vis_data_all(x_snack, y_snack, title, save=False)\n",
    "x_snack.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## knn/svm on tste embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 0.5\n",
    "Kernel = 'Gaussian'\n",
    "Gamma = 0.5\n",
    "\n",
    "k_range = [1, 3]\n",
    "m_range = list(range(3, 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'prostatex'\n",
    "model = 'dwac'\n",
    "name = 'emb10.merged2'\n",
    "train_path = '{}_train_{}.pkl'.format(model, name)\n",
    "valid_path = '{}_valid_{}.pkl'.format(model, name)\n",
    "f_train, y_train, X_train = pickle.load(open(train_path, \"rb\"))\n",
    "f_valid, y_valid, X_valid = pickle.load(open(valid_path, \"rb\"))\n",
    "data = X_train, y_train, X_valid, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwac_f_scores_knn, dwac_f_score_svm = utils.get_full_score(data, k_range)\n",
    "dwac_r_means_knn, dwac_r_confs_knn, dwac_r_means_svm, dwac_r_confs_svm = utils.get_random_score(data, k_range, m_range)\n",
    "dwac_p_scores_knn, dwac_p_scores_svm = utils.get_protodash_score(data, k_range, m_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, len(k_range), figsize=(16, 6), sharey=True)\n",
    "for k in range(len(k_range)):\n",
    "    ax[k].axhline(dwac_f_scores_knn[k], c='black')\n",
    "    ax[k].plot(m_range, dwac_r_means_knn[k])\n",
    "    ax[k].fill_between(m_range, dwac_r_means_knn[k] + dwac_r_confs_knn[k] / 2, dwac_r_means_knn[k] - dwac_r_confs_knn[k] / 2, alpha=0.5)\n",
    "    ax[k].plot(m_range, dwac_p_scores_knn[k])\n",
    "    ax[k].set_ylim(0.3, 1)\n",
    "    ax[k].set_xticks(m_range)\n",
    "    ax[k].set_title('K={}'.format(k_range[k]))\n",
    "    ax[k].legend(['full', 'random', '', 'protodash', 'pdash_e', 'proto_g', 'protoclass', 'lpips'])\n",
    "fig.suptitle('{}, {}.{}, AUC, KNN weights '.format(dataset, model, name) + \"uniform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= \"tste\"\n",
    "train_path = '{}_train_px.pkl'.format(model)\n",
    "valid_path = '{}_valid_px.pkl'.format(model)\n",
    "f_train_tste, y_train_tste, x_train_tste = pickle.load(open(train_path, \"rb\"))\n",
    "f_valid_tste, y_valid_tste, x_valid_tste = pickle.load(open(valid_path, \"rb\"))\n",
    "\n",
    "data = x_train_tste, y_train_tste, x_valid_tste, y_valid_tste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tste_f_scores_knn, tste_f_score_svm = utils.get_full_score(data, k_range)\n",
    "tste_r_means_knn, tste_r_confs_knn, tste_r_means_svm, tste_r_confs_svm = utils.get_random_score(data, k_range, m_range)\n",
    "tste_p_scores_knn, tste_p_scores_svm = utils.get_protodash_score(data, k_range, m_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, len(k_range), figsize=(16, 6), sharey=True)\n",
    "for k in range(len(k_range)):\n",
    "    ax[k].axhline(tste_f_scores_knn[k], c='black')\n",
    "    ax[k].plot(m_range, tste_r_means_knn[k])\n",
    "    ax[k].fill_between(m_range, tste_r_means_knn[k] + tste_r_confs_knn[k] / 2, tste_r_means_knn[k] - tste_r_confs_knn[k] / 2, alpha=0.5)\n",
    "    ax[k].plot(m_range, tste_p_scores_knn[k])\n",
    "    ax[k].set_ylim(0.3, 1)\n",
    "    ax[k].set_xticks(m_range)\n",
    "    ax[k].set_title('K={}'.format(k_range[k]))\n",
    "    ax[k].legend(['full', 'random', \"\", 'tste', 'pdash_e', 'proto_g', 'protoclass', 'lpips'])\n",
    "fig.suptitle('{}, {}, AUC, KNN weights '.format(dataset, model) + \"uniform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, len(k_range), figsize=(16, 6), sharey=True)\n",
    "for k in range(len(k_range)):\n",
    "    ax[k].plot(m_range, dwac_p_scores_knn[k])\n",
    "    ax[k].plot(m_range, tste_p_scores_knn[k])\n",
    "    ax[k].set_ylim(0.3, 1)\n",
    "    ax[k].set_xticks(m_range)\n",
    "    ax[k].set_title('K={}'.format(k_range[k]))\n",
    "    ax[k].legend(['protodash', 'tste'])\n",
    "fig.suptitle('{}, dwac vs tste, AUC, KNN weights uniform'.format(dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('han')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c73f886271c839d0ba877ba8b97f5003c6c6417a734903c0face75895daee34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
