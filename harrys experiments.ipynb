{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os, pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torch\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import models.utils as utils\n",
    "import csv\n",
    "\n",
    "from torchvision import transforms\n",
    "import evals.embed_evals as evals\n",
    "import pickle\n",
    "import utils.plot_data as plot\n",
    "import shutil\n",
    "import utils.gen_triplets as gen\n",
    "import pandas as pd\n",
    "from omegaconf import OmegaConf as oc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## direct NI comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter out correct NIs\n",
    "NIs = pd.read_csv(\"noise_NIs.csv\")\n",
    "RESN_NIs = pickle.load(open(\"embeds/wv_2d/RESN_d=50/RESN_NIs.pkl\",\"rb\"))\n",
    "y_train = np.array([x[1] for x in torchvision.datasets.ImageFolder(\"datasets/weevil_vespula/train\")])\n",
    "y_test = np.array([x[1] for x in torchvision.datasets.ImageFolder(\"datasets/weevil_vespula/test\")])\n",
    "syn_x_train = pickle.load(open(\"/net/scratch/tianh/explain_teach/embeds/wv_2d/train.pkl\",\"rb\"))\n",
    "syn_x_test = pickle.load(open(\"/net/scratch/tianh/explain_teach/embeds/wv_2d/test.pkl\",\"rb\"))\n",
    "def str2list(S): return [int(x) for x in S.replace(\"]\",\" \").replace(\"[\",\" \").replace(\",\",\"\").strip().split(\" \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [1,0]\n",
    "nn_comparison = []\n",
    "for NI in NIs[\"NI\"]:\n",
    "    NI = str2list(NI)\n",
    "    MTL_count, RESN_count, ties = evals.nn_comparison(syn_x_train, syn_x_test, NI, RESN_NIs[0], weights)\n",
    "    nn_comparison.append(MTL_count/40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "NIs.insert(4,\"nn_comparison\",nn_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "NIs.to_csv(\"noise_NIs.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "NIs = []\n",
    "NI = []\n",
    "a = open(\"temp/wv_NI.txt\",\"r\")\n",
    "for line in a:\n",
    "    line = line.replace(\"]\",\" \").replace(\"[\",\" \").strip().split(\" \")\n",
    "    # print(line)\n",
    "    for x in line:\n",
    "        if len(NI)==40:\n",
    "            NIs.append(NI)\n",
    "            NI=[]\n",
    "        if x.isdigit():\n",
    "            NI.append(int(x))\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## resn eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "resn_dir = \"embeds/wv_2d/RESN_d=50\"\n",
    "train_embeds = [pickle.load(open(f\"{resn_dir}/MTL_han_train_seed{seed}.pkl\",\"rb\")) for seed in range(10)]\n",
    "test_embeds = [pickle.load(open(f\"{resn_dir}/MTL_han_test_seed{seed}.pkl\",\"rb\")) for seed in range(10)]\n",
    "y_train = np.array([x[1] for x in torchvision.datasets.ImageFolder(\"datasets/weevil_vespula/train\")])\n",
    "y_test = np.array([x[1] for x in torchvision.datasets.ImageFolder(\"datasets/weevil_vespula/test\")])\n",
    "syn_x_train = pickle.load(open(\"/net/scratch/tianh/explain_teach/embeds/wv_2d/train.pkl\",\"rb\"))\n",
    "syn_x_test = pickle.load(open(\"/net/scratch/tianh/explain_teach/embeds/wv_2d/test.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "powers = [2,2]\n",
    "w2s = [0,0.0001,0.001,0.0025,0.005,0.05,1]\n",
    "aligns = [0.7,0.775,0.8,0.85,0.9,0.95,1]\n",
    "metric = [\"NINO_ds_acc\",\"rNINO_ds_acc\",\"NIFO_ds_acc\"]\n",
    "align_results = pd.DataFrame()\n",
    "NIs = []\n",
    "for w2,align in zip(w2s,aligns):\n",
    "    weights = [1,w2]\n",
    "    results = []\n",
    "    for z_train,z_test in zip(train_embeds,test_embeds):\n",
    "        data = evals.syn_evals(z_train, y_train, z_test, y_test, syn_x_train, syn_x_test, weights, powers)\n",
    "        NI = data[\"NIs\"]\n",
    "        data = {m: [data[m]] for m in metric}\n",
    "        results.append(pd.DataFrame(data=data))\n",
    "    NIs.append(NI)\n",
    "    results = pd.concat(results)\n",
    "    results.insert(0, \"align\", [align]*10)\n",
    "    align_results = pd.concat([align_results, results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(np.array(NIs),open(\"embeds/wv_2d/RESN_d=50/RESN_NIs.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>align</th>\n",
       "      <th>NINO_ds_acc</th>\n",
       "      <th>rNINO_ds_acc</th>\n",
       "      <th>NIFO_ds_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    align  NINO_ds_acc  rNINO_ds_acc  NIFO_ds_acc\n",
       "0     0.7        0.750         0.750        0.800\n",
       "0     0.7        0.800         0.800        0.600\n",
       "0     0.7        0.650         0.650        0.750\n",
       "0     0.7        0.600         0.600        0.700\n",
       "0     0.7        0.600         0.600        0.750\n",
       "..    ...          ...           ...          ...\n",
       "0     1.0        0.950         0.900        0.875\n",
       "0     1.0        0.775         0.775        0.775\n",
       "0     1.0        0.925         0.925        0.950\n",
       "0     1.0        0.900         0.850        0.850\n",
       "0     1.0        0.900         0.850        0.850\n",
       "\n",
       "[70 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "align_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### triplet gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeds = pickle.load(open(\"/net/scratch/tianh/explain_teach/embeds/wv_2d/train.pkl\",\"rb\"))\n",
    "valid_embeds = pickle.load(open(\"/net/scratch/tianh/explain_teach/embeds/wv_2d/valid.pkl\",\"rb\"))\n",
    "test_embeds = pickle.load(open(\"/net/scratch/tianh/explain_teach/embeds/wv_2d/test.pkl\",\"rb\"))\n",
    "len_train = len(train_embeds)\n",
    "len_valid = len(valid_embeds)\n",
    "len_test = len(test_embeds)\n",
    "y_train = np.array([x[1] for x in torchvision.datasets.ImageFolder(\"datasets/weevil_vespula/train\")])\n",
    "y_valid = np.array([x[1] for x in torchvision.datasets.ImageFolder(\"datasets/weevil_vespula/valid\")])\n",
    "y_test = np.array([x[1] for x in torchvision.datasets.ImageFolder(\"datasets/weevil_vespula/test\")])\n",
    "classes = [0,1]\n",
    "y_train_idx = [np.where(y_train==c)[0] for c in classes]\n",
    "y_valid_idx = [np.where(y_valid==c)[0] for c in classes]\n",
    "y_test_idx = [np.where(y_test==c)[0] for c in classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2s = [0, 0.0001,0.001,0.0025,0.005,0.05,0.5,1]\n",
    "aligns = [0.7, 0.775,0.8,0.85,0.9,0.95,0.975,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.872 0.88 0.8474\n",
      "0.85925 0.87 0.8542\n",
      "0.86975 0.875 0.8542\n",
      "0.8685 0.875 0.8426\n",
      "0.87925 0.884 0.8464\n",
      "0.88975 0.872 0.8594\n",
      "0.93825 0.929 0.9262\n",
      "0.9535 0.933 0.9458\n"
     ]
    }
   ],
   "source": [
    "for w2,align in zip(w2s, aligns):\n",
    "    dir = f\"align={str(align)}\"\n",
    "    weights = [1,w2]\n",
    "    total = 10000//2\n",
    "    train_triplets = gen.sample_triplets(np.arange(len_train), int(total*0.8), weights, train_embeds, [2,2])\n",
    "    valid_triplets = gen.sample_mixed_triplets(np.arange(len_valid),np.arange(len_train), int(total*0.2), weights, valid_embeds, train_embeds, [2,2])\n",
    "    test_triplets = gen.sample_mixed_triplets(np.arange(len_test),np.arange(len_train), total, weights, test_embeds, train_embeds, [2,2])\n",
    "    pickle.dump(train_triplets, open(f\"/net/scratch/tianh/explain_teach/datasets/wv_2d_triplets/{dir}/train_triplets.pkl\", \"wb\"))\n",
    "    pickle.dump(valid_triplets, open(f\"/net/scratch/tianh/explain_teach/datasets/wv_2d_triplets/{dir}/valid_triplets.pkl\", \"wb\"))\n",
    "    pickle.dump(test_triplets, open(f\"/net/scratch/tianh/explain_teach/datasets/wv_2d_triplets/{dir}/test_triplets.pkl\", \"wb\"))\n",
    "\n",
    "    train_triplets_filtered = gen.filter_train_triplets(train_triplets, y_train)\n",
    "    valid_triplets_filtered = gen.filter_mixed_triplets(valid_triplets, y_train, y_valid)\n",
    "    test_triplets_filtered = gen.filter_mixed_triplets(test_triplets, y_train, y_test)\n",
    "    print(len(train_triplets_filtered)/4000, len(valid_triplets_filtered)/1000, len(test_triplets_filtered)/5000)\n",
    "    pickle.dump(train_triplets_filtered, open(f\"datasets/wv_2d_triplets/{dir}/train_triplets_filtered.pkl\",\"wb\"))\n",
    "    pickle.dump(valid_triplets_filtered, open(f\"datasets/wv_2d_triplets/{dir}/valid_triplets_filtered.pkl\",\"wb\"))\n",
    "    pickle.dump(test_triplets_filtered, open(f\"datasets/wv_2d_triplets/{dir}/test_triplets_filtered.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triplets = pickle.load(open(\"datasets/wv_2d_triplets/align=0.7/train_triplets.pkl\",\"rb\"))\n",
    "valid_triplets = pickle.load(open(\"datasets/wv_2d_triplets/align=0.7/valid_triplets.pkl\",\"rb\"))\n",
    "test_triplets = pickle.load(open(\"datasets/wv_2d_triplets/align=0.7/test_triplets.pkl\",\"rb\"))\n",
    "for p in [0.05,0.1,0.15,0.3, 0.5]:\n",
    "    dir = os.path.join(\"datasets/wv_2d_triplets/align=0.7_noisy\",str(p))\n",
    "    train_noisy = gen.get_noisy_triplets(train_triplets, p)\n",
    "    valid_noisy = gen.get_noisy_triplets(valid_triplets, p)\n",
    "    test_noisy = gen.get_noisy_triplets(test_triplets, p)\n",
    "    pickle.dump(train_noisy, open(f\"{dir}/train_triplets_noisy.pkl\",\"wb\"))\n",
    "    pickle.dump(valid_noisy, open(f\"{dir}/valid_triplets_noisy.pkl\",\"wb\"))\n",
    "    pickle.dump(test_noisy, open(f\"{dir}/test_triplets_noisy.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = oc.load(\"models/configs/wv_2d/triplets/align=0.7.yaml\")\n",
    "base_filtered = oc.load(\"models/configs/wv_2d/triplets/align=0.7_filtered.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w2,align in zip(w2s, aligns):\n",
    "    weights = [1,w2]\n",
    "    train_triplets = base.train_triplets.replace(\"0.7\",str(align))\n",
    "    valid_triplets = base.valid_triplets.replace(\"0.7\",str(align))\n",
    "    test_triplets = base.test_triplets.replace(\"0.7\",str(align))\n",
    "    wandb_group = base.wandb_group.replace(\"0.7\",str(align))\n",
    "    overwrite = {\"weights\":weights, \"train_triplets\":train_triplets,\"valid_triplets\":valid_triplets,\n",
    "    \"test_triplets\":test_triplets,\"wandb_group\":wandb_group}\n",
    "    oc.save(oc.merge(base,overwrite), f\"models/configs/wv_2d/triplets/align={align}.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w2,align in zip(w2s, aligns):\n",
    "    weights = [1,w2]\n",
    "    train_triplets = base_filtered.train_triplets.replace(\"0.7\",str(align))\n",
    "    valid_triplets = base_filtered.valid_triplets.replace(\"0.7\",str(align))\n",
    "    test_triplets = base_filtered.test_triplets.replace(\"0.7\",str(align))\n",
    "    wandb_group = base_filtered.wandb_group.replace(\"0.7\",str(align))\n",
    "    overwrite = {\"weights\":weights, \"train_triplets\":train_triplets,\"valid_triplets\":valid_triplets,\n",
    "    \"test_triplets\":test_triplets,\"wandb_group\":wandb_group}\n",
    "    oc.save(oc.merge(base_filtered,overwrite), f\"models/configs/wv_2d/triplets/align={align}_filtered.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1, c2, c3, c4 = np.arange(1583), np.arange(1583,3166), np.arange(600,900), np.arange(900,1200)\n",
    "splits = utils.cross_val_multiclass([c1,c2], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = np.arange(1583)\n",
    "valid_test = np.random.choice(c1, 300,replace=False)\n",
    "c1_valid = np.random.choice(valid_test, 150, replace=False)\n",
    "c1_test = np.setdiff1d(valid_test, c1_valid)\n",
    "c1_train = np.setdiff1d(c1, valid_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2 = np.arange(1583,5856)\n",
    "valid_test = np.random.choice(c2, 300,replace=False)\n",
    "c2_valid = np.random.choice(valid_test, 150, replace=False)\n",
    "c2_test = np.setdiff1d(valid_test, c2_valid)\n",
    "c2_train = np.setdiff1d(c2, valid_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(splits,open(\"models/pneumonia_splits_imb.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = \"/net/scratch/tianh-shared/warblers/inat/4class\"\n",
    "dst_dir = \"/net/scratch/tianh-shared/warblers/data\"\n",
    "split5 = pickle.load(open(\"models/bird_splits.pkl\",\"rb\"))[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = split5\n",
    "instances = utils.dataset_filenames(src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [v[1] for v in instances[train]]\n",
    "unique = set(labels)\n",
    "counts = [labels.count(c) for c in unique]\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.arange(0.1, 1, 0.1)\n",
    "clf = [0.875, 0.925, 0.95, 0.975, 0.975, 0.975, 1, 1, 0.975]\n",
    "triplet = [0.897,0.913,0.913,0.855,0.841,0.826,0.841,0.841,0.841]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lw=4\n",
    "plt.rc('legend', fontsize=15)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.axhline(1, c='black',linewidth=lw, label=\"ResNET clf acc\")\n",
    "plt.plot(lambdas, clf,linewidth=lw, label=\"MTL clf acc\")\n",
    "plt.plot(lambdas, triplet,linewidth=lw, label=\"MTL triplet acc\")\n",
    "plt.axhline(0.841, c='green',linewidth=lw, label=\"TN triplet acc\")\n",
    "plt.xlabel('lambda')\n",
    "# plt.title(\"MTL trained on human triplets\")\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.1, -0.1),fancybox=True, shadow=True, ncol=4)\n",
    "plt.savefig(\"figs/MTL_lambdas.pdf\", format=\"pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "triplets = pickle.load(open(\"/net/scratch/tianh/triplet-webapp/backup/trial0/butterfly.triplets.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = pickle.load(open(\"data/bm_train_idx.pkl\", \"rb\"))[1]\n",
    "val_idx  = pickle.load(open(\"data/bm_valid_idx.pkl\", \"rb\"))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triplets = []\n",
    "val_triplets = []\n",
    "for triplet in triplets:\n",
    "    a,p,n = triplet\n",
    "    if a in train_idx.keys() and p in train_idx.keys() and n in train_idx.keys():\n",
    "        new = [train_idx[a], train_idx[p], train_idx[n]]\n",
    "        train_triplets.append(new)\n",
    "    elif a in val_idx.keys() and p in val_idx.keys() and n in val_idx.keys():\n",
    "        new = [val_idx[a], val_idx[p], val_idx[n]]\n",
    "        val_triplets.append(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_triplets = \"/net/scratch/tianh/explain_teach/data/bm_triplets/3c2_unique=182/train_triplets.pkl\"\n",
    "valid_triplets = \"/net/scratch/tianh/explain_teach/data/bm_triplets/3c2_unique=182/valid_triplets.pkl\"\n",
    "train_triplets = pickle.load(open(train_triplets, \"rb\"))\n",
    "valid_triplets = pickle.load(open(valid_triplets, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_lpips_distance = \"/net/scratch/tianh/explain_teach/embeds/lpips/lpips.bm.train.pkl\"\n",
    "valid_lpips_distance = \"/net/scratch/tianh/explain_teach/embeds/lpips/lpips.bm.valid.pkl\"\n",
    "train_lpips_distance = pickle.load(open(train_lpips_distance, \"rb\"))\n",
    "valid_lpips_distance = pickle.load(open(valid_lpips_distance, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for htriplet in train_triplets:\n",
    "    total += 1\n",
    "    a,p,n = htriplet[0], htriplet[1], htriplet[2]\n",
    "    if train_lpips_distance[a, p] < train_lpips_distance[a, n]:\n",
    "        correct += 1\n",
    "correct/total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for htriplet in valid_triplets:\n",
    "    total += 1\n",
    "    a,p,n = htriplet[0], htriplet[1], htriplet[2]\n",
    "    if valid_lpips_distance[a, p] < valid_lpips_distance[a, n]:\n",
    "        correct += 1\n",
    "correct/total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_pairwise_distance= \"embeds/lpips.bm.train.pkl\" \n",
    "# valid_pairwise_distance= \"embeds/lpips.bm.valid.pkl\" \n",
    "# train_pairwise_distance = pickle.load(open(train_pairwise_distance, \"rb\"))\n",
    "# valid_pairwise_distance = pickle.load(open(valid_pairwise_distance, \"rb\"))\n",
    "\n",
    "train_triplets = \"/net/scratch/tianh/explain_teach/data/bm_triplets/train_triplets.pkl\"\n",
    "valid_triplets = \"/net/scratch/tianh/explain_teach/data/bm_triplets/valid_triplets.pkl\"\n",
    "train_triplets = pickle.load(open(train_triplets, \"rb\"))\n",
    "valid_triplets = pickle.load(open(valid_triplets, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pickle.load(open(\"embeds/TN_bm_train.pkl\",\"rb\"))\n",
    "valid = pickle.load(open(\"embeds/TN_bm_valid.pkl\",\"rb\"))\n",
    "triplets = pickle.load(open(\"data/bm_triplets/val2train_triplets.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = evals.val2train_triplet_acc(train, valid, triplets)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = evals.triplet_acc(train, train_triplets)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = evals.triplet_acc(valid, valid_triplets)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_triplets = \"/net/scratch/tianh/bm/triplets/train_triplets.pkl\"\n",
    "valid_triplets = \"/net/scratch/tianh/bm/triplets/valid_triplets.pkl\"\n",
    "# clf_train_triplets = \"/net/scratch/tianh/bm/triplets/clf_train_triplets.pkl\"\n",
    "# clf_valid_triplets = \"/net/scratch/tianh/bm/triplets/clf_valid_triplets.pkl\"\n",
    "\n",
    "train_triplets = pickle.load(open(train_triplets, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dir = \"/net/scratch/hanliu-shared/data/bm/train\"\n",
    "valid_dir = \"/net/scratch/hanliu-shared/data/bm/valid\"\n",
    "train_dataset = torchvision.datasets.ImageFolder(train_dir, transform=utils.bm_transform())\n",
    "valid_dataset = torchvision.datasets.ImageFolder(valid_dir, transform=utils.bm_transform())\n",
    "train_inputs = torch.tensor(np.array([data[0].numpy() for data in train_dataset]))\n",
    "valid_inputs = torch.tensor(np.array([data[0].numpy() for data in valid_dataset]))\n",
    "train_labels = torch.tensor(np.array([data[1] for data in train_dataset]))\n",
    "valid_labels = torch.tensor(np.array([data[1] for data in valid_dataset]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(train_labels==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pickle.load(open(\"/net/scratch/tianh/bm/triplets/clf_train_triplets.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.choice(len(combs), 32, replace=False)\n",
    "sample = combs[idx]\n",
    "len(np.unique(sample.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.tensor(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.unique(torch.flatten(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_idx = []\n",
    "for c in combs:\n",
    "    x1, x2, x3 = c[0], c[1], c[2]\n",
    "    if train_pairwise_distance[x1, x2] > train_pairwise_distance[x1, x3]:\n",
    "        triplet_idx.append([x1, x3, x2])\n",
    "    else:\n",
    "        triplet_idx.append([x1, x2, x3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(np.array(triplet_idx), open('/net/scratch/tianh/bm/triplets/train_triplets.pkl',\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pickle.load(open('/net/scratch/tianh/bm/triplets/train_triplets.pkl',\"rb\"))\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(triplet_idx).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food100 = []\n",
    "with open('/net/scratch/tianh/food100-dataset/all-triplets.csv', newline='') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "    for row in spamreader:\n",
    "        food100.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize([230,230]),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "data_dir = '/net/scratch/tianh/food100-dataset/images'\n",
    "dataset = torchvision.datasets.ImageFolder(data_dir, transform=transform)\n",
    "total_idx = np.arange(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_idx = np.random.choice(total_idx, int(len(total_idx)*0.615), replace=False)\n",
    "valid_img_idx = np.setdiff1d(total_idx, train_img_idx)\n",
    "\n",
    "train_data = torch.tensor(np.array([dataset[i][0].numpy() for i in train_img_idx]))\n",
    "valid_data = torch.tensor(np.array([dataset[i][0].numpy() for i in valid_img_idx]))\n",
    "train_label = torch.tensor(np.array([dataset[i][1] for i in train_img_idx]))\n",
    "valid_label = torch.tensor(np.array([dataset[i][1] for i in valid_img_idx]))\n",
    "\n",
    "valid_triplets = []\n",
    "for t in triplets:\n",
    "    if t[0] in valid_label and t[1] in valid_label and t[2] in valid_label:\n",
    "        valid_triplets.append(t)\n",
    "train_triplets = []\n",
    "for t in triplets:\n",
    "    if t[0] in train_label and t[1] in train_label and t[2] in train_label:\n",
    "        train_triplets.append(t)\n",
    "len(train_triplets), len(valid_triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "41685/11018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(triplets)*0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets = pickle.load(open(\"/net/scratch/tianh/food100-dataset/triplets_idx.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_idx = np.arange(100)\n",
    "train_idx = np.random.choice(total_idx, len(total_idx)*8//10, replace=False)\n",
    "valid_idx = np.setdiff1d(total_idx, train_idx)\n",
    "train_triplets = 0\n",
    "valid_triplets = 0\n",
    "for t in triplets:\n",
    "    if t[0] in train_idx:\n",
    "        train_triplets += 1\n",
    "    else:\n",
    "        valid_triplets += 1\n",
    "train_triplets/len(triplets), valid_triplets/len(triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/net/scratch/tianh/food100-dataset/images\"\n",
    "dir_list = os.listdir(path)\n",
    "dir_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "for image in dir_list:\n",
    "    image_dir = os.path.join(path, image[:-4]) \n",
    "    if not os.path.exists(image_dir):\n",
    "        os.mkdir(image_dir) \n",
    "        shutil.copy(os.path.join(path,image),image_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluating lpips(bm) by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for i in range(80):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for j in range(80):\n",
    "        for k in range(80,160):\n",
    "            total += 1\n",
    "            if train_dist[i,j] <= train_dist[i,k]:\n",
    "                correct += 1\n",
    "    scores.append(correct/total)\n",
    "    \n",
    "for i in range(80, 160):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for j in range(80):\n",
    "        for k in range(80,160):\n",
    "            total += 1\n",
    "            if train_dist[i,j] >=  train_dist[i,k]:\n",
    "                correct += 1\n",
    "    scores.append(correct/total)\n",
    "sum(scores)/len(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluating lpips(food) w/ triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_matrix = pickle.load(open(\"lpips.food.pkl\", \"rb\"))\n",
    "triplets = pickle.load(open(\"/net/scratch/tianh/food100-dataset/triplets_idx.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(triplets)\n",
    "correct = 0\n",
    "for triplet in triplets:\n",
    "    if distances_matrix[triplet[0], triplet[1]] <= distances_matrix[triplet[0], triplet[2]]:\n",
    "        correct += 1\n",
    "correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## triplet generalization error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import euclidean_distances \n",
    "import torch\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "pdist = torch.nn.PairwiseDistance()\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "np.random.seed(42)\n",
    "# def euc_dist(x, y): return euclidean_distances([x],[y])[0][0]\n",
    "def euc_dist(x, y): return np.sqrt(np.dot(x, x) - 2 * np.dot(x, y) + np.dot(y, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a(f):\n",
    "    return f(3)\n",
    "def f(x): return x+1\n",
    "a(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tste\n",
    "triplets = \"/net/scratch/tianh/bm/triplets/train_triplets.pkl\"\n",
    "triplets = np.array(pickle.load(open(triplets, \"rb\")))\n",
    "embedding = tste.tste(triplets, no_dims=2, verbose=False, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_align = []\n",
    "for triplet in tqdm(triplets):\n",
    "    a, p, n = triplet\n",
    "    ap = euc_dist(embedding[a], embedding[p]) \n",
    "    an = euc_dist(embedding[a], embedding[n])\n",
    "    train_align.append(ap < an)\n",
    "print(np.mean(train_align))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets = np.array(pickle.load(open(\"/net/scratch/tianh/food100-dataset/triplets_idx.pkl\", \"rb\")))\n",
    "valid_path = \"model/embeds/triplet_net_food_triplet.pkl\"\n",
    "X_train = pickle.load(open(valid_path, \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_align = []\n",
    "for triplet in tqdm(triplets):\n",
    "    a, p, n = triplet\n",
    "    ap = euc_dist(X_train[a], X_train[p]) \n",
    "    an = euc_dist(X_train[a], X_train[n])\n",
    "    train_align.append(ap < an)\n",
    "print(np.mean(train_align))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'triplet_net_food'\n",
    "name = 'no_linear'\n",
    "train_path = \"/net/scratch/hanliu-shared/data/bm/embs/dwac_train_emb10.merged2.pkl\"\n",
    "valid_path = \"/net/scratch/hanliu-shared/data/bm/embs/dwac_valid_emb10.merged2.pkl\"\n",
    "# train_path = '{}/{}_train_{}.pkl'.format(\"embeds\", model, name)\n",
    "# valid_path = '{}/{}_valid_{}.pkl'.format(\"embeds\", model, name)\n",
    "X_train = pickle.load(open(train_path, \"rb\"))\n",
    "X_train = X_train[3]\n",
    "X_valid = pickle.load(open(valid_path, \"rb\"))\n",
    "X_valid=X_valid[3]\n",
    "# valid_path = 'model/xd.pkl'\n",
    "# X_train = pickle.load(open(valid_path, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_align = []\n",
    "train_dist = pickle.load(open(\"embeds/lpips.bm.train.pkl\", \"rb\"))\n",
    "# train_dist = train_dist[np.array(subset_idx)]\n",
    "combs = torch.combinations(torch.arange(0, len(train_dist)).int(), r=3)\n",
    "for c in tqdm(combs):\n",
    "    a, p, n = c\n",
    "# for i in tqdm(range(10000)):\n",
    "#     a, p, n = np.random.choice(len(X_train), 3, replace=False)\n",
    "    ap = train_dist[a, p] < train_dist[a, n]\n",
    "    rd = euc_dist(X_train[a], X_train[p]) < euc_dist(X_train[a], X_train[n])\n",
    "    train_align.append(ap == rd)\n",
    "print(np.mean(train_align))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_align = []\n",
    "valid_dist = pickle.load(open(\"embeds/lpips.bm.valid.pkl\", \"rb\"))\n",
    "combs = torch.combinations(torch.arange(0, len(valid_dist)-1).int(), r=3)\n",
    "for c in combs:\n",
    "    a, p, n = c\n",
    "    ap = valid_dist[a, p] < valid_dist[a, n]\n",
    "    rd = euc_dist(X_valid[a], X_valid[p]) < euc_dist(X_valid[a], X_valid[n])\n",
    "    valid_align.append(ap == rd)\n",
    "print(np.mean(valid_align))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ap = pdist(torch.tensor(X_valid[combs[:,0]]), torch.tensor(X_valid[combs[:,1]]))\n",
    "d_an = pdist(torch.tensor(X_valid[combs[:,0]]), torch.tensor(X_valid[combs[:,2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(d_ap < d_an).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## human-compatible example selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, name = 'dwac', 'emb10.merged2'\n",
    "title = \"prostatex\"\n",
    "\n",
    "train_path = 'embeds/{}_train_{}.pkl'.format(model, name)\n",
    "f_train_dwac, y_train_dwac, x_train_dwac = pickle.load(open(train_path, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, pdash_examples, _ = pdash(x_train_dwac,x_train_dwac,10,kernelType=\"Gaussian\")\n",
    "pdash_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dwac-lpips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpips = pickle.load(open(\"embeds/lpips.prostatex.train+valid.pkl\",\"rb\"))\n",
    "lpips.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, examples, _ = pdash_human.pdash(x_train_dwac,x_train_dwac, 10,lpips,kernelType=\"Gaussian\")\n",
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, examples, _ = pdash_human.pdash(x_train_dwac,x_train_dwac, 10,lpips, f_h_scale=0.1, kernelType=\"Gaussian\")\n",
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, examples, _ = pdash_human.pdash(x_train_dwac,x_train_dwac, 10,lpips, f_h_scale=1, kernelType=\"Gaussian\")\n",
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.vis_proto(x_train_dwac, y_train_dwac, examples, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.vis_proto(x_train_dwac, y_train_dwac, pdash_examples, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 0.5\n",
    "Kernel = 'Gaussian'\n",
    "Gamma = 0.5\n",
    "k_range = [1, 3]\n",
    "m_range = list(range(3, 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'prostatex'\n",
    "model = 'dwac'\n",
    "name = 'emb10.merged2'\n",
    "train_path = 'embeds/{}_train_{}.pkl'.format(model, name)\n",
    "valid_path = 'embeds/{}_valid_{}.pkl'.format(model, name)\n",
    "f_train, y_train, X_train = pickle.load(open(train_path, \"rb\"))\n",
    "f_valid, y_valid, X_valid = pickle.load(open(valid_path, \"rb\"))\n",
    "data = X_train, y_train, X_valid, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwac_f_scores_knn, dwac_f_score_svm = utils.get_full_score(data, k_range)\n",
    "dwac_r_means_knn, dwac_r_confs_knn, dwac_r_means_svm, dwac_r_confs_svm = utils.get_random_score(data, k_range, m_range)\n",
    "dwac_p_scores_knn, dwac_p_scores_svm = utils.get_protodash_score(data, k_range, m_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdash_score(data, k_range, m_range, f_h_scale):\n",
    "    p_idss = {}\n",
    "    X_train, y_train, X_valid, y_valid = data\n",
    "    for m in m_range:\n",
    "        if m not in p_idss:\n",
    "            try:\n",
    "                _, index, _ = pdash_human.pdash(X_train,X_train, m,lpips, f_h_scale=f_h_scale, kernelType=\"Gaussian\")\n",
    "            except AttributeError:\n",
    "                index = [0] * m\n",
    "                print(\"error for m={}\".format(m))\n",
    "            p_idss[m] = index\n",
    "    p_scores_knn, p_scores_svm = [], []\n",
    "    for k in k_range:\n",
    "        for m in m_range:\n",
    "            p_scores_knn.append(utils.get_knn_score(k, data, p_idss[m]))\n",
    "            try:\n",
    "                s = utils.get_svm_score(k, data, p_idss[m])\n",
    "            except:\n",
    "                s = 0\n",
    "            p_scores_svm.append(s)\n",
    "    p_scores_knn = np.array(p_scores_knn).reshape(len(k_range), len(m_range))\n",
    "    p_scores_svm = np.array(p_scores_svm).reshape(len(k_range), len(m_range))\n",
    "    return p_scores_knn, p_scores_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdash_scores_knn1,pdash_scores_svm = get_pdash_score(data, k_range, m_range, 1)\n",
    "pdash_scores_knn01,pdash_scores_svm = get_pdash_score(data, k_range, m_range, 0.1)\n",
    "pdash_scores_knn001,pdash_scores_svm = get_pdash_score(data, k_range, m_range, 0.01)\n",
    "pdash_scores_knn0001,pdash_scores_svm = get_pdash_score(data, k_range, m_range, 0.001)\n",
    "pdash_scores_knn00001,pdash_scores_svm = get_pdash_score(data, k_range, m_range, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, len(k_range), figsize=(16, 6), sharey=True)\n",
    "for k in range(len(k_range)):\n",
    "    ax[k].axhline(dwac_f_scores_knn[k], c='black')\n",
    "    ax[k].plot(m_range, dwac_r_means_knn[k])\n",
    "    ax[k].fill_between(m_range, dwac_r_means_knn[k] + dwac_r_confs_knn[k] / 2, dwac_r_means_knn[k] - dwac_r_confs_knn[k] / 2, alpha=0.5)\n",
    "    ax[k].plot(m_range, dwac_p_scores_knn[k])\n",
    "    # ax[k].plot(m_range, pdash_scores_knn1[k])\n",
    "    ax[k].plot(m_range, pdash_scores_knn01[k])\n",
    "    ax[k].plot(m_range, pdash_scores_knn001[k])\n",
    "    # ax[k].plot(m_range, pdash_scores_knn0001[k])\n",
    "    # ax[k].plot(m_range, pdash_scores_knn00001[k])\n",
    "    ax[k].set_ylim(0.3, 1)\n",
    "    ax[k].set_xticks(m_range)\n",
    "    ax[k].set_title('K={}'.format(k_range[k]))\n",
    "    ax[k].legend(['full', 'random', '', 'protodash', '1NN_constraint', 'pdash_h','0.0001'])\n",
    "fig.suptitle('{}, {}.{}, AUC, KNN weights '.format(dataset, model, \"lpips\") + \"uniform\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate triplets from LPIPS distance matrices\n",
    "triplets.shape := [k, 3]\n",
    "\n",
    "triplets[i] = [anchor, positive, negative]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_matrix = pickle.load(open(\"lpips.prostatex.train+valid.pkl\", \"rb\"))\n",
    "distance_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets_fname = \"data/triplets.px.train+valid.pkl\"\n",
    "tste_fname = \"data/tste.px.train+valid.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeds = utils.get_tste(distance_matrix, triplets_fname, tste_fname, max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, name = 'dwac', 'emb10.merged2'\n",
    "title = \"prostatex\"\n",
    "\n",
    "train_path = '{}_train_{}.pkl'.format(model, name)\n",
    "f_train_dwac, y_train_dwac, x_train_dwac = pickle.load(open(train_path, \"rb\"))\n",
    "valid_path = '{}_valid_{}.pkl'.format(model, name)\n",
    "f_valid_dwac, y_valid_dwac, x_valid_dwac = pickle.load(open(valid_path, \"rb\"))\n",
    "vis_data(x_train_dwac, y_train_dwac, x_valid_dwac, y_valid_dwac, title, save=False)\n",
    "x_train_dwac.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"prostatex, tste\"\n",
    "model, name = \"tste\", \"px\"\n",
    "\n",
    "train_path = '{}_train_{}.pkl'.format(model, name)\n",
    "f_train_tste, y_train_tste, x_train_tste = pickle.load(open(train_path, \"rb\"))\n",
    "valid_path = '{}_valid_{}.pkl'.format(model, name)\n",
    "f_valid_tste, y_valid_tste, x_valid_tste = pickle.load(open(valid_path, \"rb\"))\n",
    "vis_data(x_train_tste, y_train_tste, x_valid_tste, y_valid_tste, title, save=False)\n",
    "x_train_tste.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_snack = np.load(\"embeds/snack_dwac+lpips.npy\")\n",
    "y_snack_path = \"embeds/tste_all_px.pkl\"\n",
    "y_snack = pickle.load(open(y_snack_path, \"rb\"))[1]\n",
    "assert (x_snack.shape[0]==y_snack.shape[0])\n",
    "vis_data_all(x_snack, y_snack, title, save=False)\n",
    "x_snack.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## knn/svm on tste embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 0.5\n",
    "Kernel = 'Gaussian'\n",
    "Gamma = 0.5\n",
    "\n",
    "k_range = [1, 3]\n",
    "m_range = list(range(3, 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'prostatex'\n",
    "model = 'dwac'\n",
    "name = 'emb10.merged2'\n",
    "train_path = '{}_train_{}.pkl'.format(model, name)\n",
    "valid_path = '{}_valid_{}.pkl'.format(model, name)\n",
    "f_train, y_train, X_train = pickle.load(open(train_path, \"rb\"))\n",
    "f_valid, y_valid, X_valid = pickle.load(open(valid_path, \"rb\"))\n",
    "data = X_train, y_train, X_valid, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwac_f_scores_knn, dwac_f_score_svm = utils.get_full_score(data, k_range)\n",
    "dwac_r_means_knn, dwac_r_confs_knn, dwac_r_means_svm, dwac_r_confs_svm = utils.get_random_score(data, k_range, m_range)\n",
    "dwac_p_scores_knn, dwac_p_scores_svm = utils.get_protodash_score(data, k_range, m_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, len(k_range), figsize=(16, 6), sharey=True)\n",
    "for k in range(len(k_range)):\n",
    "    ax[k].axhline(dwac_f_scores_knn[k], c='black')\n",
    "    ax[k].plot(m_range, dwac_r_means_knn[k])\n",
    "    ax[k].fill_between(m_range, dwac_r_means_knn[k] + dwac_r_confs_knn[k] / 2, dwac_r_means_knn[k] - dwac_r_confs_knn[k] / 2, alpha=0.5)\n",
    "    ax[k].plot(m_range, dwac_p_scores_knn[k])\n",
    "    ax[k].set_ylim(0.3, 1)\n",
    "    ax[k].set_xticks(m_range)\n",
    "    ax[k].set_title('K={}'.format(k_range[k]))\n",
    "    ax[k].legend(['full', 'random', '', 'protodash', 'pdash_e', 'proto_g', 'protoclass', 'lpips'])\n",
    "fig.suptitle('{}, {}.{}, AUC, KNN weights '.format(dataset, model, name) + \"uniform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= \"tste\"\n",
    "train_path = '{}_train_px.pkl'.format(model)\n",
    "valid_path = '{}_valid_px.pkl'.format(model)\n",
    "f_train_tste, y_train_tste, x_train_tste = pickle.load(open(train_path, \"rb\"))\n",
    "f_valid_tste, y_valid_tste, x_valid_tste = pickle.load(open(valid_path, \"rb\"))\n",
    "\n",
    "data = x_train_tste, y_train_tste, x_valid_tste, y_valid_tste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tste_f_scores_knn, tste_f_score_svm = utils.get_full_score(data, k_range)\n",
    "tste_r_means_knn, tste_r_confs_knn, tste_r_means_svm, tste_r_confs_svm = utils.get_random_score(data, k_range, m_range)\n",
    "tste_p_scores_knn, tste_p_scores_svm = utils.get_protodash_score(data, k_range, m_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, len(k_range), figsize=(16, 6), sharey=True)\n",
    "for k in range(len(k_range)):\n",
    "    ax[k].axhline(tste_f_scores_knn[k], c='black')\n",
    "    ax[k].plot(m_range, tste_r_means_knn[k])\n",
    "    ax[k].fill_between(m_range, tste_r_means_knn[k] + tste_r_confs_knn[k] / 2, tste_r_means_knn[k] - tste_r_confs_knn[k] / 2, alpha=0.5)\n",
    "    ax[k].plot(m_range, tste_p_scores_knn[k])\n",
    "    ax[k].set_ylim(0.3, 1)\n",
    "    ax[k].set_xticks(m_range)\n",
    "    ax[k].set_title('K={}'.format(k_range[k]))\n",
    "    ax[k].legend(['full', 'random', \"\", 'tste', 'pdash_e', 'proto_g', 'protoclass', 'lpips'])\n",
    "fig.suptitle('{}, {}, AUC, KNN weights '.format(dataset, model) + \"uniform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, len(k_range), figsize=(16, 6), sharey=True)\n",
    "for k in range(len(k_range)):\n",
    "    ax[k].plot(m_range, dwac_p_scores_knn[k])\n",
    "    ax[k].plot(m_range, tste_p_scores_knn[k])\n",
    "    ax[k].set_ylim(0.3, 1)\n",
    "    ax[k].set_xticks(m_range)\n",
    "    ax[k].set_title('K={}'.format(k_range[k]))\n",
    "    ax[k].legend(['protodash', 'tste'])\n",
    "fig.suptitle('{}, dwac vs tste, AUC, KNN weights uniform'.format(dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('han')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c73f886271c839d0ba877ba8b97f5003c6c6417a734903c0face75895daee34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
